---
title: "Hacker News Links Analysis"
author: Szymon Lipiński
output:
  html_document:
    df_print: paged
    highlight: pygments
    css: css.css
    toc: true
    toc_depth: 2
  pdf_document: default
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
showAll <- FALSE

library(ggplot2)
library(RPostgreSQL)
library(knitr)
library(ggthemr)
library(dplyr)
library(httr)
library(rvest)
library(jsonlite)

dust = define_palette(
  background = '#FAF7F2', 
  #text = c(inner = '#5b4f41', outer = '#5b4f41'), 
  #line = c(inner = '#8d7a64', outer = '#8d7a64'),
  gridline = '#E3DDCC',
  swatch = c(
    '#555555',
    '#db735c', '#EFA86E', 
    '#9A8A76', '#F3C57B', 
    '#7A6752', '#2A91A2',
    '#87F28A', '#6EDCEF',
    '#e6efb6', '#386bd8'
  ),
  gradient = c(low = '#F3C57B', high = '#386bd8')
)
ggthemr(dust, type="outer", layout="scientific", spacing=2)
```

```{r data_processing, child = "data_processing.Rmd", echo=showAll, eval=showAll}
```

# Protocol Distribution

The URLs are gathered regardless the field type, we count one URL per object_id. So, if the same URL appears in the same entry in the title and the description, then it's counted as one. However, if it's in a story and in a comment to this story, then they are counted as two separate URLs.

```{r message=showAll, warning=showAll, echo=showAll}

drv <- dbDriver("PostgreSQL")
con <- dbConnect(drv, dbname = "hn",
                 host = "localhost", port = 5432,
                 user = "hn", password = "hn")

```


```{r message=showAll, warning=showAll, echo=showAll, cache=TRUE}

urls <- dbGetQuery(con, "
  WITH distinct_urls AS (
    SELECT DISTINCT object_id, protocol, url
    FROM urls
    UNION
    SELECT DISTINCT object_id, 'all',
           domain || '/' || path as url
    FROM urls
  )
  SELECT protocol, year_month date, count(*)
  FROM distinct_urls
  JOIN dates USING (object_id)
  WHERE date < date_trunc('month', now())
  GROUP BY protocol, year_month
  ORDER BY year_month desc, protocol
")

```


```{r echo=showAll, message=showAll, warning=showAll, cache=TRUE}

dateRange <- dbGetQuery(con, "SELECT min(year) min, max(year) max FROM dates")
breaks <- mapply({function(year) sprintf("%s-01", year)},
                 seq(dateRange$min[1], dateRange$max[1]))

gg <- ggplot(data = urls, aes(x = date, y = count, group = protocol)) +
  geom_point(aes(color = protocol)) +
  geom_smooth(method = "loess", se = FALSE, aes(color = protocol)) +  
  labs(title = "Protocol Distribution For URLs By Month",
       x = "Date",
       y = "Count",
       color = "Protocol Type") +
  scale_x_discrete(breaks = breaks) + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) 

plot(gg)

```

The number of the https links started growing very fast in 2011. In the middle of 2014 the number of http links started decreasing. What's more interesting, the http links haven't disappeared, yet.

```{r echo=showAll, message=showAll, warning=showAll, cache=TRUE}

entries <- dbGetQuery(con, "
  WITH all_entries as (
      SELECT year_month date, data.type, count(data.object_id)
      FROM data
      JOIN dates USING(object_id) 
      WHERE data.type in ('story', 'comment')
        AND date < date_trunc('month', now())
      GROUP BY year_month, data.type
  ),
  entries_with_comments as (
      SELECT year_month date, data.type, count(data.object_id)
      FROM data
      JOIN urls USING(object_id) 
      JOIN dates USING(object_id) 
      WHERE data.type in ('story', 'comment')
        AND date < date_trunc('month', now())
      GROUP BY year_month, data.type
  )
  SELECT e.date, e.type, e.count allCount, c.count commentsCount, 
         100.0 * c.count / e.count percent
  FROM all_entries e
  JOIN entries_with_comments c USING (date, type)
  ORDER BY e.date, e.type
")

gg <- ggplot(data = entries, aes(x = date, y = allcount, group = type)) +
  geom_point(aes(color = type)) +
  geom_smooth(method = "loess", se = FALSE, aes(color = type)) +  
  labs(title = "Number of Entries By Month",
       x = "Date",
       y = "Count",
       color = "Entry Type") +
  scale_x_discrete(breaks = breaks) + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1))
plot(gg)

```

The number of comments for a month is growing almost lineary, while the number of stories is almost the same for the last couple of years.

```{r echo=showAll, message=showAll, warning=showAll, cache=TRUE}

gg <- ggplot(data = entries, aes(x = date, y = commentscount, group = type)) +
  geom_point(aes(color = type)) +
  geom_smooth(method = "loess", se = FALSE, aes(color = type)) +  
  labs(title = "Entries With URLs By Month",
       x = "Date",
       y = "Count",
       color = "Entry Type") +
  scale_x_discrete(breaks = breaks) + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1))
plot(gg)
```

The number of stories and the number of comments with links is also growing, which can be caused by the growth of the entries.

```{r echo=showAll, message=showAll, warning=showAll, cache=TRUE}

gg <- ggplot(data = entries, aes(x = date, y = percent, group = type)) +
  geom_point(aes(color = type)) +
  geom_smooth(method = "loess", se = FALSE, aes(color = type)) +  
  labs(title = "Percentage of Entries With URLs By Month",
       x = "Date",
       y = "Percent",
       color = "Entry Type") +
  scale_x_discrete(breaks = breaks) + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1))
plot(gg)

```

Almost every story has a link and less than 25% of the comments.

The percentage of the stories and the coments with links is almost the same. It looks like the growth of the entries with links, shown on the previous charts, is caused by overall growth of the number of entries.

# The Most Active Domains

```{r echo=showAll, message=showAll, warning=showAll, cache=TRUE}

mostActiveDomainsByMonth <- dbGetQuery(con, "
  WITH domains AS (
      SELECT domain_without_www, count(*)
      FROM urls
      GROUP BY domain_without_www
      ORDER BY count DESC
      LIMIT 10
  )
  SELECT domain_without_www as domain, year_month date, count(*)
  FROM urls
  JOIN dates USING (object_id)
  WHERE date < date_trunc('month', now())
    AND domain in (SELECT domain_without_www FROM domains)
  GROUP BY domain_without_www, year_month
  ORDER BY domain_without_www, year_month
  ;
")

domainsWithMostPointsByMonth <- dbGetQuery(con, "
  WITH domains AS (
      SELECT domain_without_www, sum(coalesce(points, 0)) points
      FROM urls
      JOIN data USING (object_id)
      GROUP BY domain_without_www
      ORDER BY points DESC
      LIMIT 10
  )
  SELECT domain_without_www as domain, year_month date, sum(coalesce(points, 0)) points
  FROM urls
  JOIN data USING (object_id)
  JOIN dates USING (object_id)
  WHERE date < date_trunc('month', now())
    AND domain in (SELECT domain_without_www FROM domains)
  GROUP BY domain_without_www, year_month
  ORDER BY domain_without_www, year_month
;
")

mostActiveDomains <- dbGetQuery(con, "
      SELECT domain_without_www, count(*)
      FROM urls
      GROUP BY domain_without_www
      ORDER BY count DESC
      LIMIT 10
")

domainsWithMostPoints <- dbGetQuery(con, "
      SELECT domain_without_www, sum(coalesce(points, 0)) points
      FROM urls
      JOIN data USING (object_id)
      GROUP BY domain_without_www
      ORDER BY points DESC
      LIMIT 10
")
```

```{r echo=showAll, message=showAll, warning=showAll, cache=TRUE}

gg <- ggplot(data = mostActiveDomainsByMonth, aes(x = date, y = count, group = domain)) +
  geom_line(aes(color = domain)) +
  labs(title = "Most Mentioned Domains By Month",
       x = "Date",
       y = "Count",
       color = "Domain") +
  scale_x_discrete(breaks = breaks) + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1))
plot(gg)
```

The domains with the biggest number of posts are:

* `r mostActiveDomains$domain[1]`
* `r mostActiveDomains$domain[2]`
* `r mostActiveDomains$domain[3]`
* `r mostActiveDomains$domain[4]`
* `r mostActiveDomains$domain[5]`

```{r echo=showAll, message=showAll, warning=showAll, cache=TRUE}

gg <- ggplot(data = domainsWithMostPointsByMonth, aes(x = date, y = points, group = domain)) +
  geom_line(aes(color = domain)) +
  labs(title = "Domains With Most Points By Month",
       x = "Date",
       y = "Points",
       color = "Domain") +
  scale_x_discrete(breaks = breaks) + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1))
plot(gg)

```

The domains with the biggest number of points all the time are:

* `r domainsWithMostPoints$domain[1]`
* `r domainsWithMostPoints$domain[2]`
* `r domainsWithMostPoints$domain[3]`
* `r domainsWithMostPoints$domain[4]`
* `r domainsWithMostPoints$domain[5]`


```{r echo=showAll, message=showAll, warning=showAll, cache=FALSE}

getDomainData <- function(con, domain) {
  res <- dbGetQuery(con, "
    SELECT path, sum(coalesce(points, 0)) point
    FROM urls
    JOIN data USING (object_id)
    WHERE (path IS NOT NULL AND path <> '/')
      AND domain_without_www = $1
    GROUP BY path
    ORDER BY point DESC 
    LIMIT 20;
  ", params = c(domain))
  res$url <- paste(paste("https://", domain, sep=""), res$path, sep="")
  res
} 
```

# XKCD :: Comics with The Biggest Number of Points

```{r echo=showAll, message=showAll, warning=showAll, cache=TRUE}

xkcdMappings <- list(
  "/standards.png",
  "/heartbleed_explanation.png",
  "/machine_learning.png",
  "/security.png",
  "/survivorship_bias.png",
  "/click_and_drag.png",
  "/password_strength.png",
  "/ten_thousand.png",
  "/instagram.png"
)
names(xkcdMappings) <- list(
  "/927", 
  "/1354",
  "/1838",
  "/538",
  "/1827",
  "/1110",
  "/936",
  "/1053",
  "/1150"
)

baseXkcdURL <- "https://imgs.xkcd.com/comics"

xkcd <- getDomainData(con, 'xkcd.com')

xkcd$embeddedUrl <- mapply({function(path) {
      mappedUrl <- if (!(path %in% names(xkcdMappings))) {
        path
      } else {
        xkcdMappings[path][1]
      }
      # This is a special case, as this comic is not stored
      # at the usual embedded URL
      if (path == "/radiation") {
        return("https://imgs.xkcd.com/blag/radiation.png")
      }
      paste(baseXkcdURL, mappedUrl, sep = "")
    }
  }, xkcd$path)
```

```{r echo=showAll, message=showAll, warning=showAll, cache=TRUE}

xkcd$html <- sprintf(
  '<span class="list-item"> <span class="image"> <a href="%s" target="<!!>">![](%s)</a> <span class="link"> %s/ </span> </span> </span>',
  xkcd$url,
  xkcd$embeddedUrl,
  xkcd$url
)
```

```{r echo=showAll, message=showAll, warning=showAll, eval=showAll, cache=TRUE}
xkcd
```


```{r echo=showAll, message=showAll, warning=showAll, cache=TRUE}
kable(xkcd$html[1:10], col.names=c(''))
```

# Wikipedia :: Pages with The Biggest Number of Points

```{r echo=showAll, message=showAll, warning=showAll, cache=TRUE}

wikiData <- getDomainData(con, 'en.wikipedia.org')

# I need to remove some of the links, as they are not interested for the analysis
# /w/index.php - it's the main wikipedia page
wikiData <- wikiData %>%
  filter(!(path %in% c("/w/index.php")))

wikiData <- wikiData[1:10, ]

# download the html pages from the wikipedia for the urls
wikiData$page <- mapply({function(url) url %>% GET() %>% content("text")}, wikiData$url)

# parse the html to get the article title
wikiData$title <- mapply({function(html) read_html(html) %>% html_nodes("#firstHeading") %>% html_text()}, wikiData$page)

# parse the html to get the article first paragraph
wikiData$firstParagraph <- mapply({function(html) {
    x <- read_html(html) %>%
      html_nodes("#mw-content-text .mw-parser-output p")
    
    x <- Filter({function (t) html_text(t) != "\n"}, x)[1]
    html_text(x)
  }
}, wikiData$page)

data <- sprintf(
  '<p class="list-item"> <span class="title"> <a href="%s" target="<!!>">%s</a> </span> <span class="first-paragraph"> %s </span> <span class="link"> <a href="%s" target="<!!>">%s</a> </span> </p>',
  wikiData$url,
  wikiData$title,
  wikiData$firstParagraph,
  wikiData$url,
  wikiData$url
)

```

```{r echo=showAll, message=showAll, warning=showAll, cache=TRUE}
# I haven't found simpler way to list the content to markdown (sometimes kable doesn't work well):
```

`r data[1]`
`r data[2]`
`r data[3]`
`r data[4]`
`r data[5]`
`r data[6]`
`r data[7]`
`r data[8]`
`r data[9]`
`r data[10]`

# Arstechnica :: Pages with The Biggest Number of Points

```{r echo=showAll, message=showAll, warning=showAll, cache=F}
# Get the links
arstechnicaData <- getDomainData(con, 'arstechnica.com')
arstechnicaData <- arstechnicaData[1:10, ]

# Download the pages
arstechnicaData$page <- mapply({function(url) url %>% GET() %>% content("text")}, arstechnicaData$url)

# parse the html to get the article title
arstechnicaData$title <- mapply({function(html) read_html(html) %>% html_nodes("h1") %>% html_text()}, arstechnicaData$page)

# parse the html to get the article first paragraph
arstechnicaData$firstParagraph <- mapply({function(html) {
    x <- read_html(html) %>%
      html_nodes(".article-content p")

    html_text(x[1])
  }
}, arstechnicaData$page)

data <- sprintf(
  '<p class="list-item"> <span class="title"> <a href="%s" target="<!!>">%s</a> </span> <span class="first-paragraph"> %s </span> <span class="link"> <a href="%s" target="<!!>">%s</a> </span> </p>',
  arstechnicaData$url,
  arstechnicaData$title,
  arstechnicaData$firstParagraph,
  arstechnicaData$url,
  arstechnicaData$url
)
```

`r data[1]`
`r data[2]`
`r data[3]`
`r data[4]`
`r data[5]`
`r data[6]`
`r data[7]`
`r data[8]`
`r data[9]`
`r data[10]`

# Github :: Pages with The Biggest Number of Points

```{r echo=showAll, message=showAll, warning=showAll, cache=TRUE}
githubData <- getDomainData(con, 'github.com')

githubData <- githubData %>%
  filter(!(path %in% c("/search", "/sponsors")))

githubData <- githubData[1:10, ]

data <- sprintf(
  '<p class="list-item"> <span class="link"> <a href="%s" target="<!!>">%s</a> </span> </p>',
  githubData$url,
  githubData$url
)
```

`r data[1]`
`r data[2]`
`r data[3]`
`r data[4]`
`r data[5]`
`r data[6]`
`r data[7]`
`r data[8]`
`r data[9]`
`r data[10]`

# Twitter :: Twits and Accounts with The Biggest Number of Points

*Sometimes adblocks don't display the twitter cards, then you will see just the links*

```{r echo=showAll, message=showAll, warning=showAll, cache=TRUE}
twitterData <- getDomainData(con, 'twitter.com')

twitterData <- twitterData %>%
  filter(!(path %in% c("/search")))

twitterData <- twitterData[1:10, ]

twitterData$html <- mapply({function(url) {
      url <- paste("https://publish.twitter.com/oembed?url=", url ,sep = "")
      url %>%
        GET() %>% 
        content(as = "text") %>% 
        fromJSON() %>%
        .$html
    }
  },
  twitterData$url)

data <- sprintf(
  '<div class="list-item"> %s <span class="link"> <a href="%s" target="<!!>">%s</a> </span> </div>',
  twitterData$html,
  twitterData$url,
  twitterData$url
)
```


`r data[1]`
`r data[2]`
`r data[3]`
`r data[4]`
`r data[5]`
`r data[6]`
`r data[7]`
`r data[8]`
`r data[9]`
`r data[10]`

```{r data_processing, child = "software_versions.Rmd", echo=showAll, eval=showAll}
```

# About Author

Made by Szymon Lipiński.

Published at https://www.simononsoftware.com/hackernews-links-analysis/

You can contact me e.g. at https://twitter.com/szymon_lipinski

