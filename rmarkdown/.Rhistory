for (i in 1 : 1000*1000) mns = c(mns, mean(rexp(40, lambda)))
hist(mns)
mns = NULL
for (i in 1 : 1000*1000) mns = c(mns, mean(rexp(40, lambda)))
hist(mns)
mns = NULL
for (i in 1 : 1000*1000) mns = c(mns, mean(rexp(40, lambda)))
hist(mns)
mns = NULL
for (i in 1 : 1000*1000) mns = c(mns, mean(rexp(40, lambda)))
hist(mns)
mns = NULL
for (i in 1 : 1000*1000) mns = c(mns, mean(rexp(40, lambda)))
hist(mns)
mean
mns = NULL
for (i in 1 : 1000*1000) mns = c(mns, mean(rexp(40, lambda)))
hist(mns)
mns = NULL
for (i in 1 : 1000*1000) mns = c(mns, mean(rexp(40, lambda)))
hist(mns)
setwd("~/projects/szymon/datasciencecoursera/statisticalinference")
?replicate
data
lambda
data
replicate(n_simulations, rexp(n, lambda))
lambda <- 0.2
n= 40
n_simulations=100
replicate(n_simulations, rexp(n, lambda))
data <- replicate(n_simulations, rexp(n, lambda))
hist(data)
lambda
n
n_simulations=1000
data <- replicate(n_simulations, rexp(n, lambda))
hist(data)
dim(data)
means <- mean(data)
hist(means)
means <- apply(data, 2, mean)
hist(means)
install.packages('knitr')
install.packages("plotly")
index
========================================================
author: Szymon Lipiński
date: September 9, 2019
autosize: true
install.packages("webshot")
library(plotly)
plot_ly(mtcars, x=mtcars$wt, y=mtcars$mpg, mode="markers")
install.packages("ggplot2")
install.packages("DBI")
install.packages('rpostgresql')
install.packages('RPostgreSQL')
RPostgreSQL?
;
RPostgreSQL
?RPostgreSQL
??RPostgreSQL
library(odbc)
install.packages('odbc')
library(odbc)
library(odbc)
install.packages('odbc')
library(odbc)
sort(unique(odbcListDrivers()[[1]]))
drv <- dbDriver("PostgreSQL")
library(RPostgreSQL)
drv <- dbDriver("PostgreSQL")
con <- dbConnect(drv, dbname = "hn",
host = "localhost", port = 5432,
user = "hn", password = hn)
con <- dbConnect(drv, dbname = "hn",
host = "localhost", port = 5432,
user = "hn", password = 'hn')
con <- dbConnect(drv, dbname = "hn",
host = "localhost", port = 5432,
user = "hn", password = "hn")
df_postgres <- dbGetQuery(con, "SELECT * from pg_table")
df_postgres <- dbGetQuery(con, "SELECT * from pg_tables")
df_postgres
df_postgres <- dbGetQuery(con, "select count(*), type, year from raw_data join dates using (object_id) group by year, type;")
df <- dbGetQuery(con, "select count(*), type, year from raw_data join dates using (object_id) group by year, type;")
plot(df)
boxplot(df)
summary(df)
boxplot(type~year, data=df)
df$type <- as.factor(df$type)
boxplot(type~year, data=df)
df$type <- as.factor(df$type)
df$year <- as.factor(df$year)
boxplot(type~year, data=df)
boxplot(type, data=df)
boxplot(type~year, data=df)
boxplot(year~type, data=df)
df
boxplot(year~type, data=df)
df$year <- as.integer(df$year)
boxplot(year~type, data=df)
df <- dbGetQuery(con, "select type, year from raw_data join dates using (object_id);")
boxplot(year~type, data=df)
---
title: "Scraping Hackernews Data"
author: Szymon Lipiński
output:
html_document:
df_print: paged
highlight: pygments
css: css.css
pdf_document: default
---
## Downloading The Data
The data is downloaded to a set of csv file using the code available at https://github.com/szymonlipinski/hackernews_dowloader.
This made the following files:
```{bash}
du -ah /home/data/hn
```
## Creating The Database Structure
All the data is too large to keep it in R in memory for processing on my machine. An alternative is to keep it in a database, I chose PostgreSQL.
The table structure for the csv data is:
```{bash echo=FALSE}
cat ../preprocessing/raw_data.create.sql
```
All the files have been loaded with:
```{bash echo=FALSE}
cat ../preprocessing/load_files.sh
```
The loading time was about 6s per file.
# Basic Data Cleaning
## Removing Duplicates
According to the documentation of the downloader program:
```
Some entries in the files are duplicated, which is basically because of the Algolia API limitations. What's more, Hackernews users can edit their entries, so when downloading the data after some time, some entries may be different. Mechanism of loading the data to a processing pipeline should update the entries when will have a duplicated entry id.
```
To remove the duplicates, I used a simple query which should create a new table without the duplicated rows. The primary key for the data is the `object_id` column, so to make things faster, I created an index, and used `distinct on`:
```{bash echo=FALSE}
cat ../preprocessing/clean_duplicates.sql
```
## Adding Indices
I also need some indices on the data table for faster searching. I omitted the text columns, except for the ones where I will use the whole text to search, like `type = 'comment'`.
```{bash echo=FALSE}
cat ../preprocessing/indices.sql
```
## Preprocessing Data
In the further data processing, I will need to repeat some data operations. To speed it up, I will calculate a couple of things and store it in the database. I like to use materialized views for this for two reasons:
1. They can be easily refreshed to recalculate the data again.
2. They don't change the original data.
### Calculating The Dates
The only date field in the data table is the `created_at_i` which is an integer with number of seconds since the Jan 1st, 1970. As I will need to aggregate dates by weeks, days of week, months, years, to decrease the query time later, I will calculate it now:
```{bash echo=FALSE}
cat ../preprocessing/dates.view.sql
```
For faster searching, I will add some indices on the above view:
```{bash echo=FALSE}
cat ../preprocessing/dates.view.indices.sql
```
## Getting URLs
I will also get all the urls from the specific fields. For now I will mark the source of the url, as it is possible that the urls distribution in stories text is different than in comments.
```{bash echo=FALSE}
cat ../preprocessing/urls.view.sql
```
For faster searching, I will add some indices on the above view:
```{bash echo=FALSE}
cat ../preprocessing/urls.view.indices.sql
```
# Database Size
The main table size with all indices:
```{r message=TRUE}
library(curlconverter)
install.packages("curlconverter")
devtools::install_github("hrbrmstr/curlconverter")
library(curlconverter)
setwd("~/projects/szymon/data/hnanalysis/rmarkdown")
?replace
?mutate
??mutate
??replace
?replace
unlink('~/projects/szymon/data/hnanalysis/rmarkdown/hnanalysis_cache', recursive = TRUE)
unlink('~/projects/szymon/data/hnanalysis/rmarkdown/hnanalysis_cache', recursive = TRUE)
library(httr)
url<-"https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)"
straighten(paste("curl", url))
make_req(straighten(paste("curl", url)))
res <- make_req(straighten(paste("curl", url)))
res
res <- make_req(straighten(paste("curl", url)))
res$data
res <- straighten(paste("curl", url))
res
req$data
GET(url)
res <- GET(url)
content(res)
content(res, "text")
content(res, "parsed")
library(rvest)
read_html(res)
doc <- read_html(res)
html_nodes(doc)
html_nodes(doc, "#first-heading")
html_nodes(doc, "#firstHeading")
html_text(html_nodes(doc, "#firstHeading"))
wikiData <- getDomainData(con, 'en.wikipedia.org')
getDomainData <- function(con, domain) {
res <- dbGetQuery(con, "
SELECT path, sum(coalesce(points, 0)) point
FROM urls
JOIN data USING (object_id)
WHERE (path IS NOT NULL AND path <> '/')
AND domain_without_www = $1
GROUP BY path
ORDER BY point DESC
LIMIT 20;
", params = c(domain))
res$url <- paste(paste("https://", domain, sep=""), res$path, sep="")
res
}
wikiData <- getDomainData(con, 'en.wikipedia.org')
drv <- dbDriver("PostgreSQL")
con <- dbConnect(drv, dbname = "hn",
host = "localhost", port = 5432,
user = "hn", password = "hn")
wikiData <- getDomainData(con, 'en.wikipedia.org')
wikiFilteredData <- filter(wikiData, !(path %in% c("/w/index.php")))
View(wikiData)
wikiFilteredData <- filter(wikiData, !(path %in% c("/w/index.php")))
wikiFilteredData <- filter(wikiData, !(wikiData$path %in% c("/w/index.php")))
library(dplyr)
wikiFilteredData <- filter(wikiData, !(path %in% c("/w/index.php")))
View(wikiData)
View(wikiFilteredData)
wikiData <- wikiData %>%
filter(!(path %in% c("/w/index.php")))
View(wikiData)
wikiData
getWikiPage <- function(df) {
url %>%
get(df$url) %>%
content("text")
}
wikiData %>%
getWikiPage()
wikiData$page %>%
getWikiPage()
url %>%
get(df$url) %>%
content("text")
getWikiPage <- function(df) {
url %>%
get(df$url) %>%
content("text")
}
wikiData$page %>%
getWikiPage()
getWikiPage <- function(url) {
url %>%
get(url) %>%
content("text")
}
wikiData$page <- getWikiPage(wikiData$url)
getWikiPage <- function(url) {
url %>%
get() %>%
content("text")
}
wikiData$page <- getWikiPage(wikiData$url)
getWikiPage <- function(url) {
url %>%
GET() %>%
content("text")
}
wikiData$page <- getWikiPage(wikiData$url)
getWikiPage <- function(url) {
GET(url) %>%
content("text")
}
wikiData$page <- getWikiPage(wikiData$url)
getWikiPage <- function(url) {
print(url)
GET(url) %>%
content("text")
}
wikiData$page <- getWikiPage(wikiData$url)
GET("https://en.wikipedia.org/wiki/Peter_Naur")
get("https://en.wikipedia.org/wiki/Peter_Naur")
library(httr)
get("https://en.wikipedia.org/wiki/Peter_Naur")
get("http://en.wikipedia.org/wiki/Peter_Naur")
GET("http://en.wikipedia.org/wiki/Peter_Naur")
getWikiPage <- function(url) {
print(url)
GET(url) %>%
content("text")
}
wikiData$page <- getWikiPage(wikiData$url)
getWikiPage <- function(url) {
print(url)
r <- GET(url)
print(r)
content(r, "text")
}
wikiData$page <- getWikiPage(wikiData$url)
getWikiPage("https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)")
getWikiPage <- function(url) {
print(url)
r <- GET(url)
print(r)
content(r, "text")
}
wikiData$page <- mapply(getWikiPage, wikiData$url)
getWikiPage <- function(url) {
r <- GET(url)
content(r, "text")
}
wikiData$page <- mapply(getWikiPage, wikiData$url)
wikiData$page <- mapply({function(url) url %>% GET() %>% content("text")}, wikiData$url)
wikiData <- wikiData[1:10]
wikiData <- wikiData[1:10, ]
dim(wikiData)
tibble()
wikiData$title <- mapply({function(html) read_html(html) %>% html_nodes(doc, "#firstHeading") %>% html_text()}, wikiData$page)
wikiData$title <- mapply({function(html) read_html(html) %>% html_nodes("#firstHeading") %>% html_text()}, wikiData$page)
View(wikiData)
wikiData$title
html_nodes(doc, "#mw-content-text")
html_nodes(doc, "#mw-content-text .mv-parser-output")
html_nodes(doc, "#mw-content-text>.mv-parser-output")
html_nodes(doc, "#mw-content-text > .mv-parser-output")
html_nodes(doc, "#mw-content-text .mv-parser-output")
html_nodes(doc, "#mw-content-text .mw-parser-output")
html_nodes(doc, "#mw-content-text .mw-parser-output p")
html_nodes(doc, "#mw-content-text .mw-parser-output p")[2]
wikiData$title <- mapply({function(html) read_html(html) %>% html_nodes("#firstHeading") %>% html_text()}, wikiData$page)
wikiData$firstParagraph <- mapply({function(html) read_html(html) %>% html_nodes(doc, "#mw-content-text .mw-parser-output p")[2] %>% html_text()}, wikiData$page)
wikiData$firstParagraph <- mapply({function(html) read_html(html)
%>% html_nodes("#mw-content-text .mw-parser-output p")[2]
%>% html_text()}, wikiData$page)
wikiData$firstParagraph <- mapply({function(html) read_html(html) %>%
html_nodes("#mw-content-text .mw-parser-output p")[2] %>%
html_text()}, wikiData$page)
wikiData$firstParagraph <- mapply({function(html) read_html(html) %>%
html_nodes("#mw-content-text .mw-parser-output p")[2] %>%
html_text()}, wikiData$page)
wikiData$firstParagraph <- mapply({function(html) read_html(html) %>%
html_nodes("#mw-content-text .mw-parser-output p") %>%
html_text()}, wikiData$page)
wikiData$firstParagraph
wikiData$firstParagraph[1]
wikiData$firstParagraph <- mapply({function(html) read_html(html) %>%
html_nodes("#mw-content-text .mw-parser-output p") %>%
html_text()[2]}, wikiData$page)
wikiData$firstParagraph <- mapply({function(html) read_html(html) %>%
html_nodes("#mw-content-text .mw-parser-output p")[2]
}, wikiData$page)
html_nodes("#mw-content-text .mw-parser-output p")
data <- wikiData$page[1]
data
h <- read_html(data)
h
html_nodes(h, "#mw-content-text .mw-parser-output p")
html_nodes(h, "#mw-content-text .mw-parser-output p")[2]
wikiData$firstParagraph <- mapply({function(html) read_html(html) %>%
html_nodes("#mw-content-text .mw-parser-output p")[2]
}, wikiData$page)
wikiData$firstParagraph <- mapply({function(html) print(html$path); read_html(html$page) %>%
html_nodes("#mw-content-text .mw-parser-output p")[2]
}, wikiData)
wikiData$firstParagraph <- mapply({function(html)
print(html$path)
read_html(html$page) %>%
html_nodes("#mw-content-text .mw-parser-output p")[2]
}, wikiData)
wikiData$firstParagraph <- mapply({function(html, url)
print(url)
read_html(html) %>%
html_nodes("#mw-content-text .mw-parser-output p")[2]
}, wikiData$page, wikiData$url)
print(html[1])
read_html(html) %>%
html_nodes("#mw-content-text .mw-parser-output p")[2]
read_html(html) %>%
html_nodes("#mw-content-text .mw-parser-output p")
read_html(html) %>%
html_nodes("#mw-content-text .mw-parser-output p")[2]
read_html(html) %>%
html_nodes("#mw-content-text .mw-parser-output p")
read_html(h) %>%
html_nodes("#mw-content-text .mw-parser-output p")
h <- read_html(data)
h
html_nodes(h, "string((/x:html/x:body/x:div[@id='content']/x:div[@id='bodyContent']//x:p)[1])")
html_nodes(h, "string((\/x:html/x:body/x:div[@id='content']/x:div[@id='bodyContent']//x:p)[1])")
html_nodes(h, "string((/x:html/x:body/x:div[@id='content']/x:div[@id='bodyContent']//x:p)[1])")
html_nodes(h, "(/x:html/x:body/x:div[@id='content']/x:div[@id='bodyContent']//x:p)[1])")
html_nodes(h, "(//x:html/x:body/x:div[@id='content']/x:div[@id='bodyContent']//x:p)[1])")
wikiData$firstParagraph <- mapply({function(html)
read_html(html) %>%
html_nodes("#mw-content-text .mw-parser-output p") %>%  .[[2]]
}, wikiData$page)
html_nodes(h, xpath ="(//x:html/x:body/x:div[@id='content']/x:div[@id='bodyContent']//x:p)[1])")
html_nodes(h, xpath ="(/x:html/x:body/x:div[@id='content']/x:div[@id='bodyContent']//x:p)[1])")
wikiData$title
html_nodes(h, "#mw-content-text .mw-parser-output p")
x <- html_nodes(h, "#mw-content-text .mw-parser-output p")
x
x[2]
wikiData$title <- mapply({function(html) read_html(html) %>% html_nodes("#firstHeading") %>% html_text()}, wikiData$page)
wikiData$firstParagraph <- mapply({function(html)
x <- read_html(html) %>%
html_nodes("#mw-content-text .mw-parser-output p")
x[2]
}, wikiData$page)
wikiData$title <- mapply({function(html) read_html(html) %>% html_nodes("#firstHeading") %>% html_text()}, wikiData$page)
wikiData$firstParagraph <- mapply({function(html)
x <- read_html(html) %>% html_nodes("#mw-content-text .mw-parser-output p")
x[2]
}, wikiData$page)
return(x[2])
wikiData$firstParagraph <- mapply({function(html)
x <- read_html(html) %>% html_nodes("#mw-content-text .mw-parser-output p")
return(x[2])
}, wikiData$page)
wikiData$firstParagraph <- mapply({function(html)
x <- read_html(html) %>% html_nodes("#mw-content-text .mw-parser-output p")
x[[2]]
}, wikiData$page)
read_html(html) %>% html_nodes("#mw-content-text .mw-parser-output p")
read_html(h) %>% html_nodes("#mw-content-text .mw-parser-output p")
h
h <- read_html(data)
data <- wikiData$page[1]
h <- read_html(data)
read_html(data) %>% html_nodes("#mw-content-text .mw-parser-output p")
read_html(data) %>% html_nodes("#mw-content-text .mw-parser-output p")[2]
x <- read_html(data) %>% html_nodes("#mw-content-text .mw-parser-output p")
x[2]
wikiData$title <- mapply({function(html) read_html(html) %>% html_nodes("#firstHeading") %>% html_text()}, wikiData$page)
wikiData$firstParagraph <- mapply({function(html)
x <- read_html(html)
x <- html_nodes(x, "#mw-content-text .mw-parser-output p")
x[2]
}, wikiData$page)
wikiData$firstParagraph <- mapply({function(html)
x <- read_html(html)
x <- html_nodes(x, "#mw-content-text .mw-parser-output p")
x[2]
}, wikiData$page)
html_text(x[2])
wikiData$firstParagraph <- mapply({function(html)
x <- read_html(html)
x <- html_nodes(x, "#mw-content-text .mw-parser-output p")
html_text(x[2])
}, wikiData$page)
wikiData$firstParagraph <- mapply({function(html)
x <- read_html(html)
x <- html_nodes(x, "#mw-content-text .mw-parser-output p")
html_text(x[2])
}, wikiData$page)
wikiData$firstParagraph <- ''
wikiData$firstParagraph <- mapply({function(html)
x <- read_html(html)
x <- html_nodes(x, "#mw-content-text .mw-parser-output p")
html_text(x[2])
}, wikiData$page)
wikiData$firstParagraph <- mapply({function(html) {
x <- read_html(html)
x <- html_nodes(x, "#mw-content-text .mw-parser-output p")
html_text(x[2])
}
}, wikiData$page)
wikiData$firstParagraph <- mapply({function(html) {
x <- read_html(html) %>%
html_nodes("#mw-content-text .mw-parser-output p")
html_text(x[2])
}
}, wikiData$page)
wikiData$firstParagraph
wikiData$firstParagraph[1]
wikiData$firstParagraph <- mapply({function(html) {
x <- read_html(html) %>%
html_nodes("#mw-content-text .mw-parser-output p")[2]
html_text(x[2])
}
}, wikiData$page)
unlink('~/projects/szymon/data/hnanalysis/rmarkdown/hnanalysis_cache', recursive = TRUE)
unlink('~/projects/szymon/data/hnanalysis/rmarkdown/hnanalysis_cache', recursive = TRUE)
unlink('~/projects/szymon/data/hnanalysis/rmarkdown/hnanalysis_cache', recursive = TRUE)
unlink('~/projects/szymon/data/hnanalysis/rmarkdown/hnanalysis_cache', recursive = TRUE)
unlink('~/projects/szymon/data/hnanalysis/rmarkdown/hnanalysis_cache', recursive = TRUE)
unlink('~/projects/szymon/data/hnanalysis/rmarkdown/hnanalysis_cache', recursive = TRUE)
unlink('~/projects/szymon/data/hnanalysis/rmarkdown/hnanalysis_cache', recursive = TRUE)
read_html(data) %>% html_nodes("#mw-content-text .mw-parser-output p")[2]
x <- read_html(data) %>% html_nodes("#mw-content-text .mw-parser-output p")
x
x[1]
length(x)
for (i in 1:len(x)) { i }
for (i in 1:length(x)) { i }
for (i in 1:length(x)) { print(i) }
for (i in 1:length(x)) { html_text(x[i]) }
for (i in 1:length(x)) { t <- html_text(x[i]); print(t); }
for (i in 1:length(x)) { t <- html_text(x[i]); if (t != "\n") return(t)  }
a <- for (i in 1:length(x)) { t <- html_text(x[i]); if (t != "\n") return(t)  }
a
?Filter
Filter({function (t) html_text(t) != "\n"}, x)
Filter({function (t) html_text(t) != "\n"}, x)[1]
unlink('~/projects/szymon/data/hnanalysis/rmarkdown/hnanalysis_cache', recursive = TRUE)
